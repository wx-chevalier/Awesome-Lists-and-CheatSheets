# Computer Vision

- [CVAT ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/opencv/cvat): Powerful and efficient Computer Vision Annotation Tool (CVAT).

- [PaddleGAN ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/PaddlePaddle/PaddleGAN): PaddlePaddle GAN library, including lots of interesting applications like DeepFake First-Order motion transfer, Mai-ha-hiÔºàËöÇËöÅÂëÄÂòø), faceswap wav2lip, picture repair, image editing, photo2cartoon, image style transfer, and so on.

- [2020~MediaPipe ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/google/mediapipe)](https://github.com/google/mediapipe): Cross-platform, customizable ML solutions for live and streaming media.

- [2024~roboflow/supervision ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/roboflow/supervision)](https://github.com/roboflow/supervision): We write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ü§ù

# Showcases

- [2023~roboflow/notebooks ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/roboflow/notebooks)](https://github.com/roboflow/notebooks): Examples and tutorials on using SOTA computer vision models and techniques. Learn everything from old-school ResNet, through YOLO and object-detection transformers like DETR, to the latest models like Grounding DINO and SAM.

## Image & Text

- [Versatile-Diffusion ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/SHI-Labs/Versatile-Diffusion): We built Versatile Diffusion (VD), the first unified multi-flow multimodal diffusion framework, as a step towards Universal Generative AI.

## ÂõæÂÉèÂ§ÑÁêÜ

- [Rembg ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/danielgatis/rembg): Rembg is a tool to remove images background. That is it.

### ÂõæÂÉèÂàÜÁ±ª

- [Nsfw JS ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/infinitered/nsfwjs): A simple JavaScript library to help you quickly identify unseemly images; all in the client's browser. NSFWJS isn't perfect, but it's pretty accurate (~90% from our test set of 15,000 test images)... and it's getting more accurate all the time.

- [DeepCreamPy ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/deeppomf/DeepCreamPy): A deep learning-based tool to automatically replace censored artwork in hentai with plausible reconstructions.

- [2023~LightGlue ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/cvg/LightGlue)](https://github.com/cvg/LightGlue): This repository hosts the inference code for LightGlue, a lightweight feature matcher with high accuracy and adaptive pruning techniques, both in the width and depth of the network, for blazing fast inference. It takes as input a set of keypoints and descriptors for each image, and returns the indices of corresponding points between them.

## Segment

- [2023~Segment Anything ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/facebookresearch/segment-anything)](https://github.com/facebookresearch/segment-anything): The Segment Anything Model (SAM) produces high quality object masks from input prompts such as points or boxes, and it can be used to generate masks for all objects in an image. It has been trained on a dataset of 11 million images and 1.1 billion masks, and has strong zero-shot performance on a variety of segmentation tasks.

  - [2023~Grounded-Segment-Anything ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/IDEA-Research/Grounded-Segment-Anything)](https://github.com/IDEA-Research/Grounded-Segment-Anything): Marrying Grounding DINO with Segment Anything & Stable Diffusion & BLIP - Automatically Detect , Segment and Generate Anything with Image and Text Inputs

  - [Magic Copy ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/kevmo314/magic-copy)](https://github.com/kevmo314/magic-copy): Magic Copy is a Chrome extension that uses Meta's Segment Anything Model to extract a foreground object from an image and copy it to the clipboard.

  - [2023~Semantic-Segment-Anything ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/fudan-zvg/Semantic-Segment-Anything)](https://github.com/fudan-zvg/Semantic-Segment-Anything): Automated dense category annotation engine that serves as the initial semantic labeling for the Segment Anything dataset (SA-1B).

  - [2023~ZrrSkywalker/Personalize-SAM ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/ZrrSkywalker/Personalize-SAM)](https://github.com/ZrrSkywalker/Personalize-SAM): How to customize SAM to automatically segment your pet dog in a photo album?

  - [2023~opengeos/segment-geospatial ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/opengeos/segment-geospatial)](https://github.com/opengeos/segment-geospatial): A Python package for segmenting geospatial data with the Segment Anything Model (SAM)

  - [2023~SysCV/sam-hq ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/SysCV/sam-hq)](https://github.com/SysCV/sam-hq): We propose HQ-SAM to upgrade SAM for high-quality zero-shot segmentation. Refer to our paper for more details. Our code and models will be released in two weeks. Stay tuned!

  - [2023~CASIA-IVA-Lab/FastSAM ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/CASIA-IVA-Lab/FastSAM)](https://github.com/CASIA-IVA-Lab/FastSAM): The Fast Segment Anything Model(FastSAM) is a CNN Segment Anything Model trained by only 2% of the SA-1B dataset published by SAM authors. The FastSAM achieve a comparable performance with the SAM method at 50√ó higher run-time speed.

  - [2023~ChaoningZhang/MobileSAM ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/ChaoningZhang/MobileSAM)](https://github.com/ChaoningZhang/MobileSAM): üìå MobileSAM paper is available at ResearchGate and arXiv. The latest version will first appear on ResearchGate, since it takes time for arXiv to update the content.

  - [2023~yformer/EfficientSAM ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/yformer/EfficientSAM)](https://github.com/yformer/EfficientSAM): Leveraged Masked Image Pretraining for Efficient Segment Anything Resources.

- [2023~Painter ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/baaivision/Painter)](https://github.com/baaivision/Painter): Painter & SegGPT Series: Vision Foundation Models from BAAI

- [2023~Inpaint-Anything ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/geekyutao/Inpaint-Anything)](https://github.com/geekyutao/Inpaint-Anything): Users can select any object in an image by clicking on it. With powerful vision models, e.g., SAM, LaMa and Stable Diffusion (SD), Inpaint Anything is able to remove the object smoothly (i.e., Remove Anything). Further, prompted by user input text, Inpaint Anything can fill the object with any desired content (i.e., Fill Anything) or replace the background of it arbitrarily (i.e., Replace Anything).

- [2023~EditAnything ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/sail-sg/EditAnything)](https://github.com/sail-sg/EditAnything): Edit anything in images powered by segment-anything, ControlNet, StableDiffusion, etc.

- [2023~GroundingDINO ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/IDEA-Research/GroundingDINO)](https://github.com/IDEA-Research/GroundingDINO): The official implementation of "Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection"

- [2023~Segment-Everything-Everywhere-All-At-Once ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/UX-Decoder/Segment-Everything-Everywhere-All-At-Once)](https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once): We introduce SEEM that can Segment Everything Everywhere with Multi-modal prompts all at once. SEEM allows users to easily segment an image using prompts of different types including visual prompts (points, marks, boxes, scribbles and image segments) and language prompts (text and audio), etc. It can also work with any combinations of prompts or generalize to custom prompts!

- [2023~Recognize_Anything-Tag2Text ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/xinyu1205/Recognize_Anything-Tag2Text)](https://github.com/xinyu1205/Recognize_Anything-Tag2Text): A Strong Image Tagging Model & Tag2Text: Guiding Vision-Language Model via Image Tagging

## Object Detection

### Âä®‰ΩúËØÜÂà´

- [2017~Detectron ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/facebookresearch/Detectron): Detectron is Facebook AI Research's software system that implements state-of-the-art object detection algorithms, including Mask R-CNN.

- [2017~Multi Object Tracker ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/adipandas/multi-object-tracker): Object detection using deep learning and multi-object tracking.

- [2018~OpenPose ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/CMU-Perceptual-Computing-Lab/openpose): Real-time multi-person keypoint detection library for body, face, hands, and foot estimation.

- [2021~CLIP ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/openai/CLIP): CLIP (Contrastive Language-Image Pre-Training) is a neural network trained on a variety of (image, text) pairs.

- [yolov5 ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/ultralytics/yolov5): This repository represents Ultralytics open-source research into future object detection methods, and incorporates our lessons learned and best practices evolved over training thousands of models on custom client datasets with our previous YOLO repository.

- [YOLOX ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/Megvii-BaseDetection/YOLOX): YOLOX is a high-performance anchor-free YOLO, exceeding yolov3~v5 with ONNX, TensorRT, ncnn, and OpenVINO supported.

- [YOLOv6 ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/meituan/YOLOv6): a single-stage object detection framework dedicated to industrial applications.

- [Handtrack.js ![code](https://ng-tech.icu/assets/code.svg)](https://victordibia.github.io/handtrack.js/#/): ÂÆÉÂèØ‰ª•ËÆ©ÂºÄÂèë‰∫∫Âëò‰ΩøÁî®ÁªèËøáËÆ≠ÁªÉÁöÑÊâãÈÉ®Ê£ÄÊµãÊ®°ÂûãÂø´ÈÄüÂàõÂª∫ÊâãÂäø‰∫§‰∫íÂéüÂûã„ÄÇ

- [MMDetection ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/open-mmlab/mmdetection): MMDetection is an open source object detection toolbox based on PyTorch. It is a part of the OpenMMLab project.

- [2022~MMYOLO ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/open-mmlab/mmyolo): MMYOLO is an open source toolbox for YOLO series algorithms based on PyTorch and MMDetection. It is a part of the OpenMMLab project.

- [detrex ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/IDEA-Research/detrex): IDEA Open Source Toolbox for Transformer Based Object Detection Algorithms

### Tracker

- [2021~ByteTrack ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/ifzhang/ByteTrack): ByteTrack: Multi-Object Tracking by Associating Every Detection Box.

- [2023~Track-Anything ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/gaomingqi/Track-Anything)](https://github.com/gaomingqi/Track-Anything): Track-Anything is a flexible and interactive tool for video object tracking and segmentation, based on Segment Anything, XMem, and E2FGVI.

## Video

- [2018~videoflow ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/videoflow/videoflow): Python framework that facilitates the quick development of complex video analysis applications and other series-processing based applications in a multiprocessing environment.

- [2021~RobustVideoMatting ![code](https://ng-tech.icu/assets/code.svg)](https://github.com/PeterL1n/RobustVideoMatting): Robust Video Matting in PyTorch, TensorFlow, TensorFlow.js, ONNX, CoreML!

- [2023~DINOv2 ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/facebookresearch/dinov2)](https://github.com/facebookresearch/dinov2): DINOv2 models produce high-performance visual features that can be directly employed with classifiers as simple as linear layers on a variety of computer vision tasks; these visual features are robust and perform well across domains without any requirement for fine-tuning. The models were pretrained on a dataset of 142 M images without using any labels or annotations.

# CV Library

- [2023~roboflow/supervision ![code](https://ng-tech.icu/assets/code.svg) ![star](https://img.shields.io/github/stars/roboflow/supervision)](https://github.com/roboflow/supervision): We write your reusable computer vision tools. Whether you need to load your dataset from your hard drive, draw detections on an image or video, or count how many detections are in a zone. You can count on us! ü§ù
