# Neural Network List

- [Deep Learning Basics: Neural Networks, Backpropagation and Stochastic Gradient Descent](http://alexminnaar.com/deep-learning-basics-neural-networks-backpropagation-and-stochastic-gradient-descent.html)

- [[ 翻译 ] 神经网络的直观解释](http://www.hackcv.com/index.php/archives/104/): 卷积神经网络的讲解非常通俗易懂。

- [Neural networks from scratch for Javascript linguists #Series#](https://parg.co/bNa): so let’s go on a journey, I’ll tell you everything I learned, some misconceptions I had, how to interpret the results, and some basic vocabulary and fun facts along the way.

- [谷歌工程师：聊一聊深度学习的 weight initialization](http://m.leiphone.com/news/201703/3qMp45aQtbxTdzmK.html)

- [A Beginner's Guide to the Mathematics of Neural Networks](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.161.3556&rep=rep1&type=pdf): In this paper I try to describe both the role of mathematics in shaping our understanding of how neural networks operate, and the curious new mathematical concepts generated by our attempts to capture neural networks in equations. My target reader being the non-expert, I will present a biased selection of relatively simple examples of neural network tasks, models and calculations, rather than try to give a full encyclopedic review-like account of the many mathematical developments in this eld.

- [ConvnetJS demo](http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html): toy 2d classification with 2-layer neural network

- [2017~Build a flexible Neural Network with Backpropagation in Python](https://parg.co/b2W)

- [2017~What is the Role of the Activation Function in a Neural Network?](http://www.kdnuggets.com/2016/08/role-activation-function-neural-network.html)

- [A Neural Network in 11 lines of Python (Part 1)](http://iamtrask.github.io/2015/07/12/basic-python-network/)

- [Mechine Learning & Algorithm 神经网络基础](http://www.cnblogs.com/maybe2030/p/5597716.html)

- [Neural Network Architectures: 神经网络架构演化浅析](http://culurciello.github.io/tech/2016/06/04/nets.html)

- [Yes you should understand backprop](https://medium.com/@karpathy/yes-you-should-understand-backprop-e2f06eab496b#.nafnz3ycy)

- [深度学习 --- 反向传播的具体案例](https://zhuanlan.zhihu.com/p/23270674)

- [基础 | 神经网络快速入门：什么是多层感知器和反向传播？](http://mp.weixin.qq.com/s?__biz=MzA3MzI4MjgzMw==&mid=2650720758&idx=1&sn=3004c425e0d427f4900a182d74bed31d&chksm=871b0d88b06c849e951469ae1ed54e5f66074d6322eb6681c85727bb8199154709c04c48c034&mpshare=1&scene=23&srcid=1125vMg6l3RZKirGuqd1sVSF#rd)

- [聊一聊深度学习的 activation function](https://zhuanlan.zhihu.com/p/25110450)

- [2015-A Neural Network in 11 lines of Python](http://iamtrask.github.io/2015/07/12/basic-python-network/): A bare bones neural network implementation to describe the inner workings of backpropagation.

- [2018~Machine Learning for Beginners: An Introduction to Neural Networks](https://victorzhou.com/blog/intro-to-neural-networks/): A simple explanation of how they work and how to implement one from scratch in Python.

- [2022~Neural Networks: Zero to Hero 🎥](https://github.com/karpathy/nn-zero-to-hero): A course on neural networks that starts all the way at the basics. The course is a series of YouTube videos where we code and train neural networks together. The Jupyter notebooks we build in the videos are then captured here inside the lectures directory. Every lecture also has a set of exercises included in the video description. (This may grow into something more respectable).

# Back Propagation | 反向传播

- [如何直观的解释 back propagation 算法？关注者 1267 被浏览 59615](https://www.zhihu.com/question/27239198)

- [2018~Hacker's guide to Neural Networks](http://karpathy.github.io/neuralnets/): You might be eager to jump right in and learn about Neural Networks, backpropagation, how they can be applied to datasets in practice, etc.

- [2018~Coding Neural Network — Forward Propagation and Backpropagtion](https://towardsdatascience.com/coding-neural-network-forward-propagation-and-backpropagtion-ccf8cf369f76): This post will be the first in a series of posts that cover implementing neural network in numpy including gradient checking, parameter initialization, L2 regularization, dropout.
