# NLP CheatSheet | 自然语言处理概览

语言是生物同类之间由于沟通需要而制定的具有统一编码解码标准的声音(图像)指令。包含手势、表情、语音等肢体语言，文字是显像符号。

自然语言通常是指一种自然地随文化演化的语言。例如英语、汉语、日语等。有别于人造语言，例如世界语、编程语言等。

自然语言处理包括自然语言理解和自然语言生成。自然语言理解是将自然语言变成计算机能够理解的语言，及非结构化文本转变为结构化信息。

自然语言处理的四大经典难题：问答、复述、文摘、翻译。理论上，这四个题目任何一个彻底解决了，其他三个也就自然解决了。

自然语言处理经历了从规则的方法到基于统计的方法。基于统计的自然语言处理方法，在数学模型上和通信就是相同的，甚至相同的。但是科学家们也是用了几十年才认识到这个问题。统计语言模型的初衷是为了解决语音识别问题，在语音识别中，计算机需要知道一个文字序列能否构成一个有意义的句子。

自然语言处理（NLP）旨在使计算机可以智能地处理人类语言，是跨越人工智能、计算科学、认知科学、信息处理和语言学的重要跨学科领域。由于计算机和人类语言之间的交互技术的进步，语音识别、对话系统、信息检索、问答和机器翻译等 NLP 应用已经开始重塑人们识别、获取和利用信息的方式。

NLP 的发展经历了三次浪潮：理性主义、经验主义和深度学习。在第一次浪潮中，理性主义方法主张设计手工制作的规则，将知识融入 NLP 系统，这种主张假设人类思维中的语言知识是通过通用继承预先固定下来的。在第二次浪潮中，经验方法假设丰富的感官输入和表面形式的可观察语言数据是必需的，并且足以使大脑学习自然语言的详细结构。因此，人们开发了概率模型来发现大型语料库中语言的规律性。在第三次浪潮中，受生物神经系统的启发，深度学习利用非线性处理的层次模型，从语言数据中学习内在表征，旨在模拟人类的认知能力。

深度学习和自然语言处理的交叉在实际任务中取得了惊人的成功。语音识别是深度学习深刻影响的第一个工业 NLP 应用。随着大规模训练数据变得可用，深度神经网络实现了比传统经验方法低得多的识别误差。深度学习在 NLP 领域的另一个成功应用是机器翻译。使用神经网络对人类语言之间的映射进行建模的端到端神经机器翻译已经证明可以大大提高翻译质量。因此，神经机器翻译已迅速成为大型科技公司（谷歌、微软、Facebook、百度等）提供的主要商业在线翻译服务的新技术。NLP 的许多其他领域，包括语言理解和对话、词法分析和解析、知识图谱、信息检索、文本问答、社交计算、语言生成和文本情感分析，也通过深度学习取得了很大的进步，掀起了 NLP 发展的第三次浪潮。如今，深度学习是应用于几乎所有 NLP 任务的主导方法。

当前的深度学习技术是从前两大浪潮发展的 NLP 技术在概念和范式上的革命。这场革命的关键支柱包括语言实体（子词、单词、短语、句子、段落、文档等）的分布式表示，通过嵌入、嵌入的语义泛化、语言的长跨深度序列建模、有效地表示从低到高的语言水平的分层网络以及端到端的深度学习方法，来共同完成许多 NLP 任务。在深度学习浪潮之前，这些都不可能，不仅是因为在之前的浪潮中缺乏大数据和强大的计算，而且同样重要的是，近年来我们错过了正确的框架，直到深度学习范式出现。

# Language Model(语言模型)

语言模型其实就是看一句话是不是正常人说出来的。这玩意很有用，比如机器翻译、语音识别得到若干候选之后，可以利用语言模型挑一个尽量靠谱的结果。在 NLP 的其它任务里也都能用到。语言模型形式化的描述就是给定一个字符串，看它是自然语言的概率 $P(w_1, w_2, …, w_t)$。$w_1$ 到 $w_t$ 依次表示这句话中的各个词。有个很简单的推论是：

$P(w*1, w_2, …, w_t) = P(w_1) \times P(w_2 | w_1) \times P(w_3 | w_1, w_2) \times … \times P(w_t | w_1, w_2, …, w*{t-1})$

常用的语言模型都是在近似地求 $P(w*t | w_1, w_2, …, w*{t-1})$。比如 n-gram 模型就是用 $P(w*t | w*{t-n+1}, …, w\_{t-1})$ 近似表示前者。

## 词表示(Word Representation)

自然语言理解的问题要转化为机器学习的问题，第一步肯定是要找一种方法把这些符号数学化。

### One-hot Representation

NLP 中最直观，也是到目前为止最常用的词表示方法是 One-hot Representation，这种方法把每个词表示为一个很长的向量。这个向量的维度是词表大小，其中绝大多数元素为 0，只有一个维度的值为 1，这个维度就代表了当前的词。

举个栗子，

“话筒”表示为 [0 0 0 **1** 0 0 0 0 0 0 0 0 0 0 0 0 …]

“麦克”表示为 [0 0 0 0 0 0 0 0 **1** 0 0 0 0 0 0 0 …]

每个词都是茫茫 0 海中的一个 1。

这种 One-hot Representation 如果采用稀疏方式存储，会是非常的简洁：也就是给每个词分配一个数字

ID。比如刚才的例子中，话筒记为 3，麦克记为 8(假设从 0 开始记)。如果要编程实现的话，用 Hash

表给每个词分配一个编号就可以了。这么简洁的表示方法配合上最大熵、SVM、CRF 等等算法已经很好地完成了 NLP 领域的各种主流任务。

当然这种表示方法也存在一个重要的问题就是“词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系，哪怕是话筒和麦克这样的同义词也不能幸免于难。

### 词向量(Distributed Representation)

而是用 **Distributed Representation**(不知道这个应该怎么翻译，因为还存在一种叫“Distributional Representation”的表示方法，又是另一个不同的概念)表示的一种低维实数向量。这种向量一般长成这个样子：[0.792, −0.177,−0.107, 0.109, −0.542, …]。维度以 50 维和 100 维比较常见。这种向量的表示不是唯一的，后文会提到目前计算出这种向量的主流方法。(个人认为)Distributed representation

最大的贡献就是让相关或者相似的词，在距离上更接近了。向量的距离可以用最传统的欧氏距离来衡量，也可以用 cos 夹角来衡量。用这种方式表示的向量，“麦克”和“话筒”的距离会远远小于“麦克”和“天气”。可能理想情况下“麦克”和“话筒”的表示应该是完全一样的，但是由于有些人会把英文名“迈克”也写成“麦克”，导致“麦克”一词带上了一些人名的语义，因此不会和“话筒”完全一致。

## 统计语言模型

传统的统计语言模型是表示语言基本单位(一般为句子)的概率分布函数，这个概率分布也就是该语言的生成模型。一般语言模型可以使用各个词语条件概率的形式表示：

$p(s)=p(w*1^T)=p(w_1,w_2,\dots,w_T)=\Pi^T*{t=1}p(w_t|Context)$

目标也可以是采用极大似然估计来求取最大化的 Log 概率的平均值，公式为

$$
\frac{1}{T}\sum^T_{t=1}\sum_{-c \le j\le c,j \ne0}log p(w_{t+j}|w_t)
$$

其中：

- $c$是训练上下文的大小。譬如$c$取值为 5 的情况下，一次就拿 5 个连续的词语进行训练。一般来说$c$越大，效果越好，但是花费的时间也会越多。
- $p(w*{t+j}|w_t)$表示$w_t$条件下出现$w*{t+j}$的概率。

其中 Context 即为上下文，根据对 Context 不同的划分方法，可以分为五大类。

### 上下文无关模型(Context=NULL)

该模型仅仅考虑当前词本身的概率，不考虑该词所对应的上下文环境。这是一种最简单，易于实现，但没有多大实际应用价值的统计语言模型。

$p(w*t|Context)=p(w_t)=\frac{N*{w_t}}{N}$

这个模型不考虑任何上下文信息，仅仅依赖于训练文本中的词频统计。它是 n-gram 模型中当 n=1 的特殊情形，所以有时也称作 Unigram Model (—元文法统计模型)。实际应用中，常被应用到一些商用语音识别系统中。

### N-Gram 模型(Context=$w*{t-n+1},w*{t-n+2},\dots,w\_{t-1}$)

N-Gram 模型时大词汇连续语音识别中常用的一种语言模型，对中文而言，我们称之为汉语语言模型(CLM, Chinese Language Model)。汉语语言模型利用上下文中相邻词间的搭配信息，在需要把连续无空格的拼音、笔画，或代表字母或笔画的数字，转换成汉字串(即句子)时，可以计算出最大概率的句子，从而实现从到汉字的自动转换，无需用户手动选择，避开了许多汉字对应一个相同的拼音(或笔画串、数字串)的重码问题。该模型基于这样一种假设，第 n 个词的出现只与前面 n-1 个词相关，而与其它任何词都不相关，整句的概率就是各个词出现的概率的乘积。这些概率可以通过直接从语料中统计 n 个词同时出现的次数得到。常用的是二元的 Bi-Gram 和三元的 Tri-Gram。

n=1 时，就是上面所说的上下文无关模型，这里 N-Gram—般认为是$N\ge2$是 的上下文相关模型。当 n=2 时，也称为 Bigram 语言模型，直观的想，在自然语 言中“白色汽车”的概率比“白色飞翔”的概率要大很多，也就是 P(汽车|白色)> P(飞翔|白色)。n>2 也类似，只是往前看 n-1 个词而不是一个词。一般 N-Gram 模型优化的目标是最大 log 似然，即：

$\Pi^T*{t=1}p_t(w_t|w*{t-n+1},w*t|w*{t-n+2},\dots,w*t|w*{t-1})$

N-Gram 模型的优点包含了前 N-1 个词所能提供的全部信息，这些信息对当前 词出现具有很强的约束力。同时因为只看 N-1 个词而不是所有词也使得模型的效率较高。这里以 Bi-Gram 做一个实例，假设语料库总的词数为 13748：

![image](http://images.cnitblog.com/blog/407700/201310/18171638-c87b895734e748ff9a188265bccbe6bb.png)

![](http://images.cnitblog.com/blog/407700/201310/18171638-c325ffe1717e4763838913964e3971fc.png)

N-Gram 语言模型也存在一些问题：

- n-gram 语言模型无法建模更远的关系，语料的不足使得无法训练更高阶的语言模型。大部分研究或工作都是使用 Trigram，就算使用高阶的模型，其统计 到的概率可信度就大打折扣，还有一些比较小的问题采用 Bigram。

* 这种模型无法建模出词之间的相似度，有时候两个具有某种相似性的词，如果一个词经常出现在某段词之后，那么也许另一个词出现在这段词后面的概率也比较大。比如“白色的汽车”经常出现，那完全可以认为“白色的轿车”也可能经常出现。

- 训练语料里面有些 n 元组没有出现过，其对应的条件概率就是 0,导致计算一整句话的概率为 0。

#### 平滑法

方法一为平滑法。最简单的方法是把每个 n 元组的出现次数加 1，那么原来出现 k 次的某个 n 元组就会记为 k+1 次，原来出现 0 次的 n 元组就会记为出现 1 次。这种也称为 Laplace 平滑。当然还有很多更复杂的其他平滑方法，其本质都 是将模型变为贝叶斯模型，通过引入先验分布打破似然一统天下的局面。而引入 先验方法的不同也就产生了很多不同的平滑方法。

#### 回退法

方法二是回退法。有点像决策树中的后剪枝方法，即如果 n 元的概率不到，那就往上回退一步，用 n-1 元的概率乘上一个权重来模拟。

# Links

- [ ] https://mp.weixin.qq.com/s/ceCopgMboWvNxl8qgmeB-Q 提取其中的序列标注与条件随机场
