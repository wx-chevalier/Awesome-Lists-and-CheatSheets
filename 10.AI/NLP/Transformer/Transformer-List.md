# Transformer List

- [2018-The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/): In this post, we will look at The Transformer â€“ a model that uses attention to boost the speed with which these models can be trained.

- [2023-The Transformer Family Version 2.0](https://lilianweng.github.io/posts/2023-01-27-the-transformer-family-v2/): Many new Transformer architecture improvements have been proposed since my last post on â€œThe Transformer Familyâ€ about three years ago. Here I did a big refactoring and enrichment of that 2020 post â€” restructure the hierarchy of sections and improve many sections with more recent papers. Version 2.0 is a superset of the old version, about twice the length.

# Resource

## Courses

- [2022-The Hugging Face Course ğŸ¥](https://github.com/huggingface/course): This repo contains the content that's used to create the Hugging Face course. The course teaches you about applying Transformers to various tasks in natural language processing and beyond. Along the way, you'll learn how to use the Hugging Face ecosystem â€” ğŸ¤— Transformers, ğŸ¤— Datasets, ğŸ¤— Tokenizers, and ğŸ¤— Accelerate â€” as well as the Hugging Face Hub. It's completely free and open-source!

## Series

- [2021-åŸºäº Transformers çš„è‡ªç„¶è¯­è¨€å¤„ç†(NLP)å…¥é—¨ #Series#](https://datawhalechina.github.io/learn-nlp-with-transformers/#/): Natural Language Processing with transformers. æœ¬é¡¹ç›®é¢å‘çš„å¯¹è±¡æ˜¯ï¼šNLP åˆå­¦è€…ã€transformer åˆå­¦è€…ï¼Œæœ‰ä¸€å®šçš„ pythonã€pytorch ç¼–ç¨‹åŸºç¡€ï¼Œå¯¹å‰æ²¿çš„ transformer æ¨¡å‹æ„Ÿå…´è¶£ï¼Œäº†è§£å’ŒçŸ¥é“ç®€å•çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚
