[![返回目录](https://parg.co/UGo)](https://github.com/wxyyxc1992/Awesome-Reference) 
 
 
# 机器学习模型评估

* [简单理解混淆矩阵](http://blog.csdn.net/songchaomail/article/details/43834741/)

* [ 混淆矩阵（Confusion Matrix ）分析 ](http://blog.csdn.net/vesper305/article/details/44927047)

* [What is a Confusion Matrix in Machine Learning](http://machinelearningmastery.com/confusion-matrix-machine-learning/)

# 正则化

* [Machine learning methodology: Overfitting,regularization, and all that](http://people.eecs.berkeley.edu/~russell/classes/cs194/f11/lectures/CS194%20Fall%202011%20Lecture%2004.pdf)

* [Wiki-Regularization (mathematics)](https://en.wikipedia.org/wiki/Regularization_%28mathematics%29#Regularization_in_statistics_and_machine_learning)

* [机器学习中的范数规则化](http://blog.csdn.net/zouxy09/article/details/24971995)

* [What-is-the-difference-between-L1-and-L2-regularization](https://www.quora.com/What-is-the-difference-between-L1-and-L2-regularization)

* [differences-between-l1-and-l2-as-loss-function-and-regularization](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)

- [一文告诉你机器学习中进行模型评价、模型选择和算法选择的终极方法（PART I ）](http://dataunion.org/24543.html)

- [differences-between-l1-and-l2-as-loss-function-and-regularization](http://www.chioka.in/differences-between-l1-and-l2-as-loss-function-and-regularization/)

- [0-norm-l1-norm-l2-norm-l-infinity-norm](https://rorasa.wordpress.com/2012/05/13/l0-norm-l1-norm-l2-norm-l-infinity-norm/)

- [discriminative-modeling-vs-generative-modeling](http://freemind.pluskid.org/machine-learning/discriminative-modeling-vs-generative-modeling/)

- [机器学习中防止过拟合的处理方法](http://blog.csdn.net/heyongluoyao8/article/details/49429629)
