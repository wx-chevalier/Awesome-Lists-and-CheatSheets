[![返回目录](https://parg.co/UGo)](https://github.com/wxyyxc1992/Awesome-Reference) 
 
 
# 文本表示

- [2016-NLP Research Lab Part 1: Distributed Representations](http://blog.districtdatalabs.com/nlp-research-lab-part-1-distributed-representations): How I Learned To Stop Worrying And Love Word Embeddings

# Word2Vec

- [The Code Word2Vec Tutorial Part I: The SkipGram Model](http://mccormickml.com/assets/word2vec/Alex_Minnaar_Word2Vec_Tutorial_Part_I_The_Skip-Gram_Model.pdf)，[Word2Vec Tutorial Part 2 - Negative Sampling](http://mccormickml.com/2017/01/11/word2vec-tutorial-part-2-negative-sampling/)

- [word2vec Explained: Deriving Mikolov et al.’s Negative-Sampling Word-Embedding Method](https://arxiv.org/pdf/1402.3722.pdf)

- [word2vec Parameter Learning Explained](https://arxiv.org/pdf/1411.2738.pdf)

- [论文翻译章节：基于Negative Sampling 的模型 ](http://blog.csdn.net/itplus/article/details/37998797)

- [word2vec: negative sampling (in layman term)?](http://stackoverflow.com/questions/27860652/word2vec-negative-sampling-in-layman-term)